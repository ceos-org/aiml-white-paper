[Previous](venues-of-cross-disciplinary-research.md) | [Table of contents](/README.md) | [Next](the-future-of-ai4eo-research.md)

***

## 3.2. From sensing to applications

**Table 3.2-1** provides a detailed breakdown of most of the work in AI4EO in various verticals. Here are the main takeaways:

* Most of the works in AI4EO investigate optical data since it is the most abundant.  
* **CNN** architectures have recently become the most ubiquitous architecture as they allow for the discovery of shift-invariant patterns not only in space, but also in time (**3D-CNNs**). They are also easier to train than **RNNs**, hence the latter are starting to be used less frequently. Therefore, CNNs are recognised as a natural fit for segmentation and classification problems in EO.  
* **SAR** data is becoming increasingly available, with many interesting use-cases emerging for InSAR.  
* With a heterogeneous array of sensors, it is clear that learning to **fuse** instruments harmoniously is key to complementing the observation gaps in isolated instruments.  
* The majority of use-cases in EO are framed as **segmentation** problems, to identify the spatial extent of an object, or regression / nowcasting / forecasting of continuous quantities, like crop yield.  
* Scientific fields have all found uses of ML to embed data into physical models (data assimilation), or fine-tuning physical models through data (parameter retrieval), or forming digital-twins of large-scale phenomena for cheaper simulation and at finer resolutions (down-scaling, subgrid-parametrization).  
* Agriculture is becoming the most frequent user of EO data (after intelligence / reconnaissance users), both in the environmental / sustainability front, like food-security, and commercially, for crop yield forecasting, monitoring, and asset management.  
* Understanding the type of some surface on the Earth (LULC) is a problem that cuts across many disciplines, and ties naturally with **classification** and **segmentation** ML tasks and **architectures**.

| proximity | sensor | method | task | application |
| ----- | ----- | ----: | ----- | ----: |
|  orbital / satellite \[[23](/reference.md#ref.23, [24](/reference.md#ref.24)\] aerial \[[25](/reference.md#ref.25),[26](/reference.md#ref.26),[27](/reference.md#ref.27), [28](/reference.md#ref.28, [29](/reference.md#ref.29), [30](/reference.md#ref.30), [31](/reference.md#ref.31)\] in-situ / ground data \[[32](/reference.md#ref.32), [33](/reference.md#ref.33), [34](/reference.md#ref.34), [35](/reference.md#ref.35)\] |  optical \[[36](/reference.md#ref.36), [37](/reference.md#ref.37), [38](/reference.md#ref38), [39](/reference.md#ref.39), [40](/reference.md#ref.40), [41](/reference.md#ref.41), [42](/reference.md#ref.42), [43](/reference.md#ref.43), [44](/reference.md#ref.44), [45](/reference.md#ref.45), [46](/reference.md#ref.46), [47](/reference.md#ref.47), [48](/reference.md#ref.48), [49](/reference.md#ref.49), [50](/reference.md#ref.50), [27](/reference.md#ref.27), [51](/reference.md#ref.51), [52](/reference.md#ref.52), [53](/reference.md#ref.53), [54](/reference.md#ref.54), [55](/reference.md#ref.55), [56](/reference.md#ref.56), [57](/reference.md#ref.57), [58](/reference.md#ref.58), [59](/reference.md#ref.59), [60](/reference.md#ref.60)\]  SAR \[[41](/reference.md#ref.41), [25](/reference.md#ref.25), [61](/reference.md#ref.61, [48](/reference.md#ref.48), [62](/reference.md#ref.62), [63](/reference.md#ref.63), [26](/reference.md#ref.26), [34](/reference.md#ref.34), [64](/reference.md#ref.64), [65](/reference.md#ref.65), [66](/reference.md#ref.66), [67](/reference.md#ref.67), [68](/reference.md#ref.68), [69](/reference.md#ref.69), [70](/reference.md#ref.70), [71](/reference.md#ref.71), [72](/reference.md#ref.72), [60](/reference.md#ref.60), [73](/reference.md#ref.73), [74](/reference.md#ref.74), [75](/reference.md#ref.75)\] lidar \[[76](/reference.md#ref.76), [41](/reference.md#ref.41), [26](/reference.md#ref.26), [77](/reference.md#ref.77), [54](/reference.md#ref.54), [78](/reference.md#ref.78), [79](/reference.md#ref.79), [80](/reference.md#ref.80), [29](/reference.md#ref.29), [30](/reference.md#ref.30), [31](/reference.md#ref.31), [81](/reference.md#ref.81)\] | autoencoder \[[82](/reference.md#ref.82), [83](/reference.md#ref.83), [84](/reference.md#ref.84), [85](/reference.md#ref.85)\]  attention / transformer \[[86](/reference.md#ref.86), [68](/reference.md#ref.68), [52](/reference.md#ref.52), [81](/reference.md#ref.81), [87](/reference.md#ref.87)\]  Bayesian / probabilistic \[[32](/reference.md#ref.32), [33](/reference.md#ref.33), [88](/reference.md#ref.88)\] CNN / U-Net / ResNet \[[39](/reference.md#ref.39), [76](/reference.md#ref.76), [40](/reference.md#ref.40), [42](/reference.md#ref.42), [45](/reference.md#ref.45), [62](/reference.md#ref.62), [50](/reference.md#ref.50), [89](/reference.md#ref.89), [90](/reference.md#ref.90), [65](/reference.md#ref.65), [68](/reference.md#ref.68), [52](/reference.md#ref.52), [91](/reference.md#ref.91), [77](/reference.md#ref.77), [28](/reference.md#ref.28), [92](/reference.md#ref.92), [54](/reference.md#ref.54), [59](/reference.md#ref.59), [81](/reference.md#ref.81)\]  ensemble \[[33](/reference.md#ref.33), [42](/reference.md#ref.42)\]  GAN \[[62](/reference.md#ref.62), [93](/reference.md#ref.93), [94](/reference.md#ref.94)\], [95](/reference.md#ref.95)\]  boosting \[[48](/reference.md#ref.48)\]  regression \[[25](/reference.md#ref.25), [96](/reference.md#ref.96), [82](/reference.md#ref.82), [43](/reference.md#ref.43), [48](/reference.md#ref.48), [62](/reference.md#ref.62), [97](/reference.md#ref.97), [98](/reference.md#ref.98)\]  MLP \[[99](/reference.md#ref.99), [42](/reference.md#ref.42), [100](/reference.md#ref.100)\]  random forest \[[61](/reference.md#ref.61), [45](/reference.md#ref.45), [48](/reference.md#ref.48), [97](/reference.md#ref.97)\]  RNN \[[32](/reference.md#ref.32), [61](/reference.md#ref.61), [47](/reference.md#ref.47), [88](/reference.md#ref.88), [50](/reference.md#ref.50), [66](/reference.md#ref.66), [92](/reference.md#ref.92), [60](/reference.md#ref.60)\]  SVM \[[61](/reference.md#ref.61), [48](/reference.md#ref.48), [62](/reference.md#ref.62), [101](/reference.md#ref.101)\] | anomaly detection \[[102](/reference.md#ref.102), [85](/reference.md#ref.85)\]  change detection \[[41](/reference.md#ref.41), [103](/reference.md#ref.103), [62](/reference.md#ref.62), [69](/reference.md#ref.69), [71](/reference.md#ref.71), [104](/reference.md#ref.104), [105](/reference.md#ref.105)\]  classification \[[36](/reference.md#ref.36), [40](/reference.md#ref.40), [42](/reference.md#ref.42), [82](/reference.md#ref.82), [106](/reference.md#ref.106), [103](/reference.md#ref.103), [44](/reference.md#ref.44), [45](/reference.md#ref.45), [47](/reference.md#ref.47), [27](/reference.md#ref.27), [65](/reference.md#ref.65), [52](/reference.md#ref.52), [91](/reference.md#ref.91), [77](/reference.md#ref.77), [75](/reference.md#ref.75)\]  data assimilation \[[96](/reference.md#ref.96), [82](/reference.md#ref.82/, [88](/reference.md#ref.88), [107](/reference.md#ref.107), [108](/reference.md#ref.108)\]  enhancement / infilling / image translation \[[103](/reference.md#ref.103), [66](/reference.md#ref.66), [109](/reference.md#ref.109), [110](/reference.md#ref.110), [57](/reference.md#ref.57), [93](/reference.md#ref.93), [94](/reference.md#ref.94), [98](/reference.md#ref.98), [95](/reference.md#ref.95)\]  object detection \[[39](/reference.md#ref.39), [106](/reference.md#ref.106), [103](/reference.md#ref.103), [50](/reference.md#ref.50), [101](/reference.md#ref.101)\]  registration / image matching \[[111](/reference.md#ref.111), [112](/reference.md#ref.112)\]  regression / forecasting \[[99](/reference.md#ref.99), [25](/reference.md#ref.25), [82](/reference.md#ref.82), [43](/reference.md#ref.43), [48](/reference.md#ref.48), [107](/reference.md#ref.107), [49](/reference.md#ref.49), [64](/reference.md#ref.64), [100](/reference.md#ref.100), [92](/reference.md#ref.92), [113](/reference.md#ref.113), [97](/reference.md#ref.97), [35](/reference.md#ref.35), [74](/reference.md#ref.64)\]  search / mining \[[36](/reference.md#ref.36), [103](/reference.md#ref.103), [114](/reference.md#ref.114), [107](/reference.md#ref.107), [108](/reference.md#ref.108/, [115](/reference.md#ref.115), [87](/reference.md#ref.87)\]  segmentation \[[36](/reference.md#ref.36), [37](/reference.md#ref.37), [39](/reference.md#ref.39), [76](/reference.md#ref.76), [40](/reference.md#ref.40), [42](/reference.md#ref.42), [82](/reference.md#ref.82), [44](/reference.md#ref.44), [116](/reference.md#ref.116/, [50](/reference.md#ref.50), [90](/reference.md#ref.90), [117](/reference.md#ref.117), [57](/reference.md#ref.57), [60](/reference.md#ref.60)\]  sensor fusion \[[41](/reference.md#ref.41), [42](/reference.md#ref.42), [82](/reference.md#ref.82), [103](/reference.md#ref.103), [88](/reference.md#ref.88), [26](/reference.md#ref.26), [108](/reference.md#ref.108), [52](/reference.md#ref.52), [113](/reference.md#ref.113), [80](/reference.md#ref.80), [81](/reference.md#ref.81), [118](/reference.md#ref.118)\]  surrogate modelling \[[99](/reference.md#ref.99), [96](/reference.md#ref.96), [108](/reference.md#ref.108), [35](/reference.md#ref.35)\]  transfer learning \[[82](/reference.md#ref.82), [43](/reference.md#ref.43), [50](/reference.md#ref.50), [91](/reference.md#ref.91)\]  unsupervised learning \[[115](/reference.md#ref.115), [53](/reference.md#ref.53), [59](/reference.md#ref.59)\] | agriculture \[[25](/reference.md#ref.25), [42](/reference.md#ref.42), [119](/reference.md#ref.119), [120](/reference.md#ref.120), [82](/reference.md#ref.82), [43](/reference.md#ref.43), [46](/reference.md#ref.46), [63](/reference.md#ref.63), [121](/reference.md#ref.121), [100](/reference.md#ref.100), [52](/reference.md#ref.52), [113](/reference.md#ref.113), [97](/reference.md#ref.97), [29](/reference.md#ref.29), [30](/reference.md#ref.30)\]  climate  \[[99](/reference.md#ref.99), [122](/reference.md#ref.122), [88](/reference.md#ref.88), [48](/reference.md#ref.48), [63](/reference.md#ref.63), [49](/reference.md#ref.49), [53](/reference.md#ref.53), [92](/reference.md#ref.92)\]  earth / geo science \[[99](/reference.md#ref.99), [96](/reference.md#ref.96), [82](/reference.md#ref.82), [88](/reference.md#ref.88), [63](/reference.md#ref.63, [107](/reference.md#ref.107), [123](/reference.md#ref.123), [108](/reference.md#ref.108), [31](/reference.md#ref.31)\]  energy \[[36](/reference.md#ref.36), [33](/reference.md#ref.33), [82](/reference.md#ref.82), [74](/reference.md#ref.74)\]  environmental science \[[82](/reference.md#ref.82), [61](/reference.md#ref.61), [34](/reference.md#ref.34)\]  epidemiology \[[51](/reference.md#ref.51)\]  finance / insurance \[[58](/reference.md#ref.58), [124](/reference.md#ref.124)\]  ship / aircraft awareness \[[39](/reference.md#ref.39), [65](/reference.md#ref.65), [73](/reference.md#ref.73), [75](/reference.md#ref.75)\] geology  \[[86](/reference.md#ref.86), [119](/reference.md#ref.119), [63](/reference.md#ref.63), [69](/reference.md#ref.69), [71](/reference.md#ref.71), [54](/reference.md#ref.54), [56](/reference.md#ref.56)\]  hydrology / hydrography \[[32](/reference.md#ref.32), [38](/reference.md#ref.38), [82](/reference.md#ref.82), [46](/reference.md#ref.46), [64](/reference.md#ref.64), [54](/reference.md#ref.54)\]  infrastructure / development \[[36](/reference.md#ref.36), [37](/reference.md#ref.37), [40](/reference.md#ref.40), [49](/reference.md#ref.49), [102](/reference.md#ref.102), [69](/reference.md#ref.69), [70](/reference.md#ref.70)\]  LULC / forestry \[[42](/reference.md#ref.42), [82](/reference.md#ref.82), [106](/reference.md#ref.102), [103](/reference.md#ref.103), [61](/reference.md#ref.61), [47](/reference.md#ref.47), [62](/reference.md#ref.62), [27](/reference.md#ref.27)\]  livestock \[[125](/reference.md#ref.125)\]  maritime \[[45](/reference.md#ref.45), [65](/reference.md#ref.65), [55](/reference.md#ref.55), [75](/reference.md#ref.75)\] |

**Table 3.2-1**: From sensing to application. A breakdown of work in AI4EO in terms of the type of sensor employed, modelling approach, prediction task, and application domain.

|  **task** | attention / transformer | CNN | RNN | GAN | MLP | random forest | regression | SVM | Bayesian / probabilistic |
| ----- | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: |
| **change detection** anomaly**,** LULC |  | \[[82](/reference.md#ref.82), [62](/reference.md#ref.62),[85](/reference.md#ref.85)\] |  |  |  |  | \[[62](/reference.md#ref.62)\] | \[[62](/reference.md#ref.62)\]  |  |
| **classification** building, damage, vehicle, hazard, scene, LULC, crop | \[[52](/reference.md#ref.52), [81](/reference.md#ref.81)\] | \[[40](/reference.md#ref.40), [42](/reference.md#ref.42), [82](/reference.md#ref.82), [45](/reference.md#ref.45), [62](/reference.md#ref.62), [65](/reference.md#ref.65), [52](/reference.md#ref.52), [91](/reference.md#ref.91), [77](/reference.md#ref.77), [28](/reference.md#ref.28)\] | \[[61](/reference.md#ref.61), [47](/reference.md#ref.47)\] |  | \[[42](/reference.md#ref.42)\] | \[[61](/reference.md#ref.61), [45](/reference.md#ref.45)\] | \[[82](/reference.md#ref.82)\] | \[[61](/reference.md#ref.61)\] |  |
| **search / mining** question answering**,** relational reasoning**,** image search | \[[87](/reference.md#ref.87)\]   |  |  |  |  |  |  |  |  |
| **Image translation** inpainting, infilling**,** super-resolution, pansharpening, deblurring, denoising, synthetic generation | \[[68](/reference.md#ref.68)\] | \[[68](/reference.md#ref.68)\] | \[[66](/reference.md#ref.66)\] | \[[93](/reference.md#ref.93), [94](/reference.md#ref.94), [95](/reference.md#ref.95)\] |  |  | \[[98](/reference.md#ref.98)\] |  |  |
| **fusion** cross-calibration, harmonisation, optical \+ optical, optical \+ lidar**,**  optical \+ SAR | \[[52](/reference.md#ref.52), [81](/reference.md#ref.81)\] | \[[42](/reference.md#ref.42), [48](/reference.md#ref.48), [52](/reference.md#ref.52), [77](/reference.md#ref.77), [54](/reference.md#ref.54), [81](/reference.md#ref.81), [118](/reference.md#ref.118),\] | \[[32](/reference.md#ref.32), [88](/reference.md#ref.88)\] |  | \[[42](/reference.md#ref.42)\] |   |  |  | \[[32](/reference.md#ref.32)\] |
| **object detection / counting** Infrastructure, vehicle, cattle |  | \[[50](/reference.md#ref.50), [89](/reference.md#ref.89)\] | \[[50](/reference.md#ref.50)\] |  |  |  |  | \[[101](/reference.md#ref.101)\] |  |
| **registration /  image matching** video stabilisation, 3D reconstruction, orthorectification |  | \[[111](/reference.md#ref.111), [112](/reference.md#ref.112)\] |  |  |  |  | \[[111](/reference.md#ref.ayi2r4cfons6), [112](/reference.md#ref.2elsyncvizn6)\] |  |  |
| **regression / forecasting** water volume**,** precipitation**,** temperature**,** velocimetry, utilisation, biomass, moisture, wind power |  |  | \[[32](/reference.md#ref.32), [82](/reference.md#ref.82)\] |  | \[[99](/reference.md#ref.99), [100](/reference.md#ref.100)\] | \[[48](/reference.md#ref.48), [97](/reference.md#ref.97)\] | \[[25](/reference.md#ref.25), [43](/reference.md#ref.43), [48](/reference.md#ref.48), [82](/reference.md#ref.82)\] | \[[48](/reference.md#ref.48), [97](/reference.md#ref.97)\] | \[[32](/reference.md#ref.32), [33](/reference.md#ref.33)\] |
| **segmentation** cloud / shadow, vegetation, building, oil slick, LULC, water, rock, road, crop | \[[86](/reference.md#ref.86) | \[[39](/reference.md#ref.39), [76](/reference.md#ref.76), [40](/reference.md#ref.40), [42](/reference.md#ref.42), [82](/reference.md#ref.82), [50](/reference.md#ref.50), [89](/reference.md#ref.89), [90](/reference.md#ref.90), [54](/reference.md#ref.54) | \[[50](/reference.md#ref.50), [60](/reference.md#ref.60)\] |  | \[[42](/reference.md#ref.42)\] |  | \[[82](/reference.md#ref.82)\] |  |  |
| **transfer learning** self-supervised, semi-supervised, zero/few-shot learning, domain adaptation |  | \[[91](/reference.md#ref.91), [59](/reference.md#ref.59) | \[[50](/reference.md#ref.50)\] |  |  |  |  |  |  |

**Table 3.2-2**: Tasks and models. Selected works in AI4EO grouped by task and modelling approach. 



***

[Previous](venues-of-cross-disciplinary-research.md) | [Table of contents](/README.md) | [Next](the-future-of-ai4eo-research.md)
